import gc
import json
import os

import numpy as np
from tqdm import tqdm

import utilities as utils

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'

import tensorflow as tf


class TagCXNModalities:  # base model class

    def __init__(self, configuration):
        self.configuration = configuration
        self.backbone = self.configuration['model']['backbone'] 
        self.preprocessor = self.configuration['model']['preprocessor']
        self.train_images_folder = self.configuration['data']['train_images_folder']
        self.val_images_folder = self.configuration['data']['val_images_folder']
        self.test_images_folder = self.configuration['data']['test_images_folder']
        self.train_data_path = self.configuration['data']['train_data_path']
        self.val_data_path = self.configuration['data']['val_data_path']
        self.test_data_path = self.configuration['data']['test_data_path']
        self.img_size = self.configuration['data']['img_size']
        self.train_data, self.val_data, self.test_data = dict(), dict(), dict()
        self.train_img_index, self.train_concepts_index = dict(), dict()
        self.val_img_index, self.val_concepts_index = dict(), dict()
        self.test_img_index = dict()
        self.tags_list = list()

        self.model = None

    def init_structures(self, skip_head=False, split_token='\t'):
        if '.csv' in self.train_data_path:
            self.train_data = self.load_csv_data(self.train_data_path, skip_head=skip_head, 
                                                 split_token=split_token)
        else:
            self.train_data = self.load_json_data(self.train_data_path)
        if '.csv' in self.val_data_path:
            self.val_data = self.load_csv_data(self.val_data_path, skip_head=skip_head,
                                               split_token=split_token)
        else:
            self.val_data = self.load_json_data(self.val_data_path)
        if '.csv' in self.test_data_path:
            self.test_data = self.load_csv_data(self.test_data_path, skip_head=skip_head, 
                                                split_token=split_token)
        else:
            self.test_data = self.load_json_data(self.test_data_path)

        print('Number of training examples:', len(self.train_data), 'Number of validation examples:',
              len(self.val_data), 'Number of testing examples:',
              len(self.test_data))

        self.train_img_index, self.train_concepts_index = utils.create_index(self.train_data)
        self.val_img_index, self.val_concepts_index = utils.create_index(self.val_data)
        self.test_img_index, _ = utils.create_index(self.test_data) 

        # self.off_test_data = list()
        # with open('../test_images.csv', 'r') as f:
        #     next(f)
        #     for line in f:
        #         self.off_test_data.append(str(line).split('\n', 1)[0])
        # print('Number of test instances:', len(self.off_test_data))
        # self.off_test_img_index = dict(zip(range(len(self.off_test_data)), list(self.off_test_data)))

        self.tags_list = self.load_tags(self.train_data)  # 5 tags!

        # remove modalities
        # modalities = ['X-Ray Computed Tomography', 'Computed Tomography', 'Ultrasonography', 'Magnetic Resonance Imaging', 'CT']
        # modalities = ['C0041618', 'C0024485', 'C0040405', 'C1699633']
        # self.tags_list = list(
        #     set(self.tags_list) - set(modalities)
        # )

        print('Number of categories:', len(self.tags_list))
        print(self.tags_list)

    @staticmethod
    def load_csv_data(file_name, skip_head=False, split_token='\t'):
        """
        loads .csv file into a Python dictionary.
        :param file_name: the path to the file (string)
        :param skip_head: whether to skip the first row of the file (if there is a header) (boolean)
        :return: data dictionary (dict)
        """
        data = dict()
        modalities = ['C0041618', 'C0024485', 'C0040405', 'C1699633', 'C0002978']
        with open(file_name, 'r') as f:
            if skip_head:
                next(f)
            for line in f:
                image = line.replace('\n', '').split(split_token)
                concepts = image[1].split(';')
                concepts = [x for x in concepts if x in modalities]
                # if len(concepts) > 1: print(image[0], concepts)
                if image[0]:
                    data[str(image[0] + '.jpg')] = ';'.join(concepts)
        print('Data loaded from:', file_name)
        return data

    @staticmethod
    def load_json_data(file_name):
        """
        loads the data of JSON format into a Python dictionary
        :param file_name: the path to the file (string)
        :return: data dictionary (dict)
        """
        print('Data loaded from:', file_name)
        og = json.load(open(file=file_name, mode='r'))
        data = dict()
        for img in og:
            if 'normal' in og[img] and len(og[img]) == 1:
                og[img].remove('normal')
            data[img] = og[img]
        return data

    @staticmethod
    def load_tags(training_data):
        """
        loads the tags list
        :param training_data: training dictionary
        :return: the tags list
        """
        # if not isinstance(tags, list):
        #     return [line.strip() for line in open(tags, 'r')]
        tags = list()
        for img in training_data:
            if isinstance(training_data[img], str):
                if training_data[img]:
                    tags.extend(training_data[img].split(';'))
            else:
                tags.extend(training_data[img])
        tags = set(tags)
        return list(tags)

    def build_model(self, pooling, repr_dropout=0., mlp_hidden_layers=None,
                    mlp_dropout=0., use_sam=False, batch_size=None, data_format='channels_last'):
        """
        builds the Keras model
        :param pooling: global pooling method (string)
        :param repr_dropout: whether to apply dropout to the encoder's representation (rate != 0) (float)
        :param mlp_hidden_layers: a list containing the
        number of units of the MLP head. Leave None for plain linear (list)
        :param mlp_dropout: whether to apply dropout to the MLPs layers (rate != 0) (float)
        :param use_sam: whether to use SAM optimization (boolean)
        :param batch_size: the batch size of training (int)
        :param data_format: whether the channels will be last
        :return: Keras model
        """
        if data_format == 'channels_first':
            inp = tf.keras.layers.Input(shape=self.img_size[::-1], name='input')
            x = self.backbone(inp, training=False).last_hidden_state
        else:
            inp = tf.keras.layers.Input(shape=self.img_size, name='input')
            x = self.backbone(self.backbone.input, training=False)

        encoder = tf.keras.Model(inputs=self.backbone.input, outputs=x, name='backbone')
        z = encoder(inp)
        if pooling == 'avg':
            z = tf.keras.layers.GlobalAveragePooling2D(name='avg_pool', data_format=data_format)(z)
        elif pooling == 'max':
            z = tf.keras.layers.GlobalMaxPooling2D(name='max_pool', data_format=data_format)(z)
        else:
            z = utils.GeM(name='gem_pool', data_format=data_format)(z)

        if repr_dropout != 0.:
            z = tf.keras.layers.Dropout(rate=repr_dropout, name='repr_dropout')(z)
        for i, units in enumerate(mlp_hidden_layers):
            z = tf.keras.layers.Dense(units=units, activation='relu', name=f'MLP-layer-{i}')(z)
            if mlp_dropout != 0.:
                z = tf.keras.layers.Dropout(rate=mlp_dropout, name=f'MLP-dropout-{i}')(z)

        z = tf.keras.layers.Dense(units=len(self.tags_list), activation='softmax', name='LR')(z)
        model = tf.keras.Model(inputs=inp, outputs=z, name='TagCXN')
        print(model.summary())
        if use_sam:
            assert batch_size // 4 == 0  # this must be divided exactly due to tf.split in the implementation of SAM.
            model = tf.keras.models.experimental.SharpnessAwareMinimization(
                model=model, num_batch_splits=(batch_size // 4), name='TagCXN_w_SAM'
            )
        return model

    def train(self, train_parameters):
        """
        method that trains the model
        :param train_parameters: model and training hyperparameters
        :return: a Keras history object
        """
        batch_size = train_parameters.get('batch_size')
        self.model = self.build_model(pooling=train_parameters.get('pooling'),
                                      repr_dropout=train_parameters.get('repr_dropout'),
                                      mlp_hidden_layers=train_parameters.get('mlp_hidden_layers'),
                                      mlp_dropout=train_parameters.get('mlp_dropout'),
                                      use_sam=train_parameters.get('use_sam'), batch_size=batch_size,
                                      data_format=self.configuration['model']['data_format'])
        # loss = None
        if train_parameters.get('loss', {}).get('name') == 'ce':
            loss = tf.keras.losses.CategoricalCrossentropy()

        self.model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=train_parameters.get('learning_rate')),
            loss=loss, metrics=[tf.keras.metrics.CategoricalAccuracy()]
        )

        early_stopping = utils.ReturnBestEarlyStopping(monitor='val_loss',
                                                       mode='min',
                                                       patience=train_parameters.get('patience_early_stopping'),
                                                       restore_best_weights=True, verbose=1)

        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.1,
                                                         patience=train_parameters.get('patience_reduce_lr'))

        print('\nTraining...')
        history = self.model.fit(
            self.train_generator(list(self.train_img_index), batch_size, self.tags_list),
            steps_per_epoch=np.ceil(len(self.train_img_index) / batch_size),
            validation_data=self.val_generator(list(self.val_img_index), batch_size, self.tags_list),
            validation_steps=np.ceil(len(self.val_img_index) / batch_size),
            callbacks=[early_stopping, reduce_lr], verbose=1, epochs=train_parameters['epochs']
        )
        print('\nEnd of training...')

        if train_parameters.get('checkpoint_path') is not None:
            self.model.save(train_parameters.get('checkpoint_path'))

        gc.collect()

        return history

    def train_generator(self, ids, batch_size, train_tags):
        """
        generator for training data
        :param ids: indices for each training sample in a batch (list)
        :param batch_size: batch size (int)
        :param train_tags: list of tags
        :return: yields a batch of data
        """
        batch = list()
        while True:
            np.random.shuffle(ids)
            for i in ids:
                batch.append(i)
                if i != len(ids):  # if not in the end of the list
                    if len(batch) == batch_size:
                        yield utils.load_batch(batch, self.train_img_index, self.train_concepts_index,
                                               self.train_images_folder, train_tags,
                                               self.preprocessor, size=self.img_size)

                        batch *= 0
                else:
                    yield utils.load_batch(batch, self.train_img_index, self.train_concepts_index,
                                           self.train_images_folder, train_tags, self.preprocessor, size=self.img_size)
                    batch *= 0

    def val_generator(self, ids, batch_size, train_tags):
        """
        generator for validation data
        :param ids: indices for each validation sample in a batch (list)
        :param batch_size: batch size (int)
        :param train_tags: list of tags
        :return: yields a batch of data
        """
        batch = list()
        while True:
            np.random.shuffle(ids)
            for i in ids:
                batch.append(i)
                if i != len(ids):
                    if len(batch) == batch_size:
                        yield utils.load_batch(batch, self.val_img_index,
                                               self.val_concepts_index, self.val_images_folder,
                                               train_tags, self.preprocessor, size=self.img_size, )
                        batch *= 0
                else:
                    yield utils.load_batch(batch, self.val_img_index, self.val_concepts_index,
                                           self.val_images_folder, train_tags, self.preprocessor, size=self.img_size, )
                    batch *= 0

    def test_generator(self, ids, index, batch_size, t='val'):
        """
        generator for testing data
        :param ids: indices for each testing sample in a batch (list)
        :param index: data index (dict)
        :param batch_size: batch size (int)
        :param t: flag for validation or testing (string)
        :return:
        """
        batch = list()
        while True:
            # np.random.shuffle(ids)
            for i in ids:
                batch.append(i)
                if i != len(ids):
                    if len(batch) == batch_size:
                        yield utils.load_test_batch(batch, index, self.val_images_folder
                                                    if t == 'val' else self.test_images_folder,
                                                    self.preprocessor, size=self.img_size)
                        batch *= 0
                else:
                    yield utils.load_test_batch(batch, index, self.val_images_folder
                                                if t == 'val' else self.test_images_folder,
                                                self.preprocessor, size=self.img_size)
                    batch *= 0

    def train_tune_test(self):
        """
        core logic of the file --> train, tune and test
        :return: a test score float, the test results in a dictionary format and a textual summary
        """
        self.init_structures(skip_head=self.configuration['data']['skip_head'], 
                             split_token=self.configuration['data']['split_token'])

        train_parameters = self.configuration['training_parameters']
        train_parameters.update(self.configuration['model_parameters'])

        training_history = self.train(train_parameters=train_parameters)

        # bs = list(utils.divisor_generator(len(self.val_img_index)))[1]
        # val_predictions = self.model.predict(self.test_generator(list(self.val_img_index),
        #                                                          self.val_img_index, bs),
        #                                      verbose=1,
        #                                      steps=np.ceil(len(self.val_img_index) / bs))
        # print(val_predictions.shape)
        # del val_predictions

        bs = list(utils.divisor_generator(len(self.test_img_index)))[1]
        test_predictions = self.model.predict(self.test_generator(list(self.test_img_index),
                                                                  self.test_img_index, bs, t='test'),
                                              verbose=1,
                                              steps=np.ceil(len(self.test_img_index) / bs))
        # test_predictions = np.argmax(test_predictions, axis=1)
        print(test_predictions.shape)
        # test_score, test_results = self.test(best_threshold=best_threshold,
        #                                      predictions=test_predictions, )
        # test_score, test_results = self.test_w_knn(best_threshold=best_threshold, 
        #                                            predictions=test_predictions)
        # test_score, test_results = self.test_hybrid(t1=0.4, t2=0.2, 
        #                                             predictions=test_predictions)
        del test_predictions


        ###### official test predictions ######
        # bs = list(utils.divisor_generator(len(self.off_test_img_index)))[1]
        # off_test_predictions = self.model.predict(self.test_generator(list(self.off_test_img_index),
        #                                                           self.off_test_img_index, bs, t='test'),
        #                                           verbose=1,
        #                                           steps=np.ceil(len(self.off_test_img_index) / bs))
        # print(off_test_predictions.shape)
        # y_pred_off_test = dict()
        # for i in tqdm(range(len(off_test_predictions))):
        #     predicted_tags = list()
        #     # bt = best_threshold
        #     for j in range(len(self.tags_list)):
        #         if off_test_predictions[i, j] >= best_threshold:
        #             predicted_tags.append(str(self.tags_list[j]))
        #     temp = ';'.join(predicted_tags)
        #     y_pred_off_test[str(list(self.off_test_data)[i]).split('.', 1)[0]] = temp
        # print('\n\nSaving official test results...\n')
        # with open('cnn_ffnn_densenet_off_test_pred_2.csv', 'w') as out_test:
        #     for result in y_pred_off_test:
        #         out_test.write(result + ',' + y_pred_off_test[result] + '\n')
        # print('Results saved!')
        # del off_test_predictions, y_pred_off_test
        ###### official test predictions ######

        # s = ('Development score = ' + str(test_score) +
        #      ' with threshold = ' + str(best_threshold) + ' and validation score = ' + str(val_score))

        # return test_score, test_results, best_threshold, s

    def run(self):
        """
        basic run method
        :return: a dictionary of checkpoint paths alongside with scores and thresholds
        """
        thresholds_map = dict()
        test_scores = list()
        info = list()

        test_score, test_results, best_threshold, txt = self.train_tune_test()
        test_scores.append(test_score)
        if self.configuration['training_parameters']['checkpoint_path'] is not None:
            thresholds_map[self.configuration['training_parameters']['checkpoint_path']] = [best_threshold, test_score]
        info.append(txt)
        for i in range(len(info)):
            print(info[i])
        # s = 'Mean dev score was: ' + str(sum(test_scores) / len(test_scores)) + '\n\n\n'
        # print(s)
        info *= 0
        test_scores *= 0
        #
        if self.configuration.get('save_results'):
            print('\n\nSaving results...\n')
            with open(self.configuration.get('results_path'), 'w') as out_test:
                for result in test_results:
                    out_test.write(str(result).split('.', 1)[0] + ',' + test_results[result] + '\n')
            print('Results saved!')

        # pickle.dump(thresholds_map, open(str(self.backbone_name) + '_map.pkl', 'wb'))
        # pickle.dump(thresholds_map, open('temp_map.pkl', 'wb'))
        return thresholds_map

    def test(self, best_threshold, predictions):
        """
        method that performs the evaluation on the test data
        :param best_threshold: the tuned classification threshold (float)
        :param predictions: 2D array of test predictions (NumPy array)
        :return: test score and test results dictionary
        """
        print('\nStarting evaluation on test set...')
        y_pred_test = dict()

        for i in tqdm(range(len(predictions))):
            predicted_tags = list()
            # bt = best_threshold
            for j in range(len(self.tags_list)):
                if predictions[i, j] >= best_threshold:
                    predicted_tags.append(str(self.tags_list[j]))

            # string! --> will be split in the f1 function

            # final_tags = list(set(set(predicted_tags).union(set(most_frequent_tags))))
            # temp = ';'.join(final_tags)
            temp = ';'.join(predicted_tags)
            y_pred_test[list(self.test_data)[i]] = temp

        f1_score, p, r, _ = utils.evaluate_f1(self.test_data, y_pred_test, test=True)
        print('\nThe F1 score on the test set is: {}\n'.format(f1_score))
        # print('Precision score:', p)
        # print('Recall score:\n', r)
        # pickle.dump(y_pred_test, open(f'my_test_results_split_{split}.pkl', 'wb'))
        return f1_score, y_pred_test
    
    def test_w_knn(self, best_threshold, predictions):
        import pickle
        from collections import Counter

        print('\nStarting evaluation on test set...')

        def get_embeddings(data):
            embeddings = dict()
            for img_name in tqdm(data):
                if 'train' in img_name:
                    images_path = '../train/'
                else:
                    images_path = '../valid/'
                path = os.path.join(images_path, img_name)
                img = tf.keras.preprocessing.image.load_img(path, target_size=self.img_size)
                img_array = tf.keras.preprocessing.image.img_to_array(img)
                img_array = np.expand_dims(img_array, axis=0)
                img_array = self.preprocessor.preprocess_input(img_array)
                embedding = self.model.predict(img_array, verbose=0).T.flatten()
                embedding = embedding / np.linalg.norm(embedding)
                embeddings[img_name] = embedding
            return embeddings
        
        def knn_retrieve_tags(best_k, best_r, test_embedding, train_embeddings):
            train_embeddings_array = np.array([train_embeddings[i] for i in train_embeddings])
            ids = [i for i in train_embeddings]
            sims = train_embeddings_array @ test_embedding.flatten()
            sims = np.array(sims).flatten()

            top_k = np.argsort(sims)[-best_k:]
            tags_list = list()
            for i, index in enumerate(top_k):
                tags = self.train_data[ids[index]].split(';')
                for tag in tags:
                    tags_list.append(tag)
            most_frequent_tags = Counter(tags_list).most_common(best_r)

            return set([t[0] for t in most_frequent_tags])

        def knn_retrieve_tags1or2(best_k, test_embedding, train_embeddings):
            # modalities = ['C0040405', 'C1699633', 'C0041618', 'C0024485']

            train_embeddings_array = np.array([train_embeddings[i] for i in train_embeddings])
            ids = [i for i in train_embeddings]
            sims = train_embeddings_array @ test_embedding.flatten()
            sims = np.array(sims).flatten()

            top_k = np.argsort(sims)[-best_k:]
            tags_list = list()
            for i, index in enumerate(top_k):
                tags = self.train_data[ids[index]].split(';')
                for tag in tags:
                    tags_list.append(tag)
            
            tag_counts = Counter(tags_list)
            top_tag, top_count = tag_counts.most_common(1)[0]

            second_tag, second_count = tag_counts.most_common(2)[-1]
            threshold = 0.58  
            if (top_count - second_count) / top_count < threshold:
                top_tags = [top_tag, second_tag]
            else:
                top_tags = [top_tag]
            
            return set(top_tags)

        def knn_retrieve_tags_more(best_k, test_embedding, train_embeddings):
            train_embeddings_array = np.array([train_embeddings[i] for i in train_embeddings])
            ids = [i for i in train_embeddings]
            sims = train_embeddings_array @ test_embedding.flatten()
            sims = np.array(sims).flatten()

            top_k = np.argsort(sims)[-best_k:]
            tags_list = list()
            for i, index in enumerate(top_k):
                tags = self.train_data[ids[index]].split(';')
                for tag in tags:
                    tags_list.append(tag)
            
            tag_counts = Counter(tags_list)
            top_tag, top_count = tag_counts.most_common(1)[0]

            second_tag, second_count = tag_counts.most_common(2)[-1]
            threshold = 0.58  
            if (top_count - second_count) / top_count < threshold:
                top_tags = [top_tag, second_tag]
            else:
                top_tags = [top_tag]
            
            threshold2 = 0.65
            # If the second and third tags are very close, include the third tag as well
            third_tag, third_count = tag_counts.most_common(3)[-1]
            if (top_count - third_count) / top_count < threshold2:
                top_tags.append(third_tag)

            # If the third and fourth tags are very close, include the fourth tag as well
            fourth_tag, fourth_count = tag_counts.most_common(4)[-1]
            if (top_count - fourth_count) / top_count < threshold2:
                top_tags.append(fourth_tag)

            return set(top_tags)

        
        # train_embeddings = get_embeddings(self.train_data)
        train_embeddings = pickle.load(open('../embeddings_foivos/embedding_dict_train_ordered2024.pkl', 'rb'))
        new_embeddings_dict_train = dict()
        for image_name, embedding in train_embeddings.items():
            new_image_name = image_name + '.jpg'
            new_embeddings_dict_train[new_image_name] = embedding.T.flatten() / np.linalg.norm(embedding.T.flatten())
        train_embeddings = dict(new_embeddings_dict_train)

        # test_embeddings = get_embeddings(self.test_data)
        test_embeddings = pickle.load(open('../embeddings_foivos/embedding_dict_dev_ordered2024.pkl', 'rb'))
        new_embeddings_dict_test = dict()
        for image_name, embedding in test_embeddings.items():
            new_image_name = image_name + '.jpg'
            new_embeddings_dict_test[new_image_name] = embedding.T.flatten() / np.linalg.norm(embedding.T.flatten())
        test_embeddings = dict(new_embeddings_dict_test)

        y_pred_test = dict()
        for i in tqdm(range(len(predictions))):
            embedding_i = test_embeddings[list(test_embeddings.keys())[i]] 
            
            predicted_tags = list()
            

            for j in range(len(self.tags_list)):
                if predictions[i, j] >= best_threshold:  # confident
                    predicted_tags.append(str(self.tags_list[j]))   
            
            if not predicted_tags:
                # knn_tags = knn_retrieve_tags(best_k=33, best_r=1, test_embedding=embedding_i, 
                #                              train_embeddings=train_embeddings)
                # knn_tags = knn_retrieve_tags1or2(best_k=33, test_embedding=embedding_i,
                #                                  train_embeddings=train_embeddings)
                knn_tags = knn_retrieve_tags_more(best_k=33, test_embedding=embedding_i,
                                                  train_embeddings=train_embeddings)
                predicted_tags.extend(list(knn_tags))
        
            y_pred_test[list(self.test_data.keys())[i]] = ';'.join(list(set(predicted_tags)))

        f1_score, p, r, _ = utils.evaluate_f1(self.test_data, y_pred_test, test=True)
        return f1_score, y_pred_test
                

                    




